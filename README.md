<div align="center">

# Awesome Retrieval-Augmented Generation

[![License: MIT](https://img.shields.io/badge/License-MIT-purple.svg)](LICENSE)
[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

</div>

## üî• News
- **2025.06**: This repository shares the latest papers, tools, and open-source projects in the field and **will be regularly updated**.

## üåü Introduction
I am a researcher specializing in RAG technology at Zhejiang University. This curated repository has been established with the aim of sharing cutting-edge papers, innovative technologies, and notable open-source projects within this domain. 

<details>
  <summary>üóÇÔ∏è Table of Contents</summary>
  <ol>
    <li><a href="#survey">Survey</a></li>
    <li><a href="#technique">Technique</a>
      <ul>
        <li><a href="#llm">Chain-of-Retrieval</a></li>
        <li><a href="#mllm">üß† Multimodal Reasoning in Large Language Models</a></li>
        <li><a href="#lm">ü§è Scaling Smaller Language Models to Reason</a></li>
      </ul>
    </li>
  </ol>
</details>

## Survey
- [2312] **[Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/pdf/2312.10997)** ![Arxiv](https://img.shields.io/badge/Arxiv-Paper-red)
  - *Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang*
  - [Code üíª] [RAG-Survey](https://github.com/Tongji-KGLLM/RAG-Survey) ![Star](https://img.shields.io/github/stars/Tongji-KGLLM/RAG-Survey.svg?style=social&label=Star)

## Technique
### Chain-of-Retrieval
- [2502] **[DeepRAG: Thinking to Retrieval Step by Step for Large Language Models](https://arxiv.org/pdf/2502.01142)** ![Arxiv](https://img.shields.io/badge/Arxiv-Paper-red)
  - *Xinyan Guan, Jiali Zeng, Fandong Meng, Chunlei Xin, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, Jie Zhou*
- [2501] **[Chain-of-Retrieval Augmented Generation](https://arxiv.org/pdf/2501.14342)** ![Arxiv](https://img.shields.io/badge/Arxiv-Paper-red)
  - *Liang Wang, Haonan Chen, Nan Yang, Xiaolong Huang, Zhicheng Dou, Furu Wei*
  - [Code üíª] [corag](https://github.com/microsoft/LMOps/tree/main/corag) ![Star](https://img.shields.io/github/stars/microsoft/LMOps.svg?style=social&label=Star)

### Active Retrieval
- [2406] **[Unified Active Retrieval for Retrieval Augmented Generation](https://arxiv.org/pdf/2406.12534)** ![Arxiv](https://img.shields.io/badge/Arxiv-Paper-red)
  - *Qinyuan Cheng, Xiaonan Li, Shimin Li, Qin Zhu, Zhangyue Yin, Yunfan Shao, Linyang Li, Tianxiang Sun, Hang Yan, Xipeng Qiu*
  - [Code üíª] [UAR](https://github.com/xiami2019/UAR) ![Star](https://img.shields.io/github/stars/xiami2019/UAR.svg?style=social&label=Star)

### Reinforcement Learning
- [2503] **[R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning](https://arxiv.org/pdf/2503.05592)** ![Arxiv](https://img.shields.io/badge/Arxiv-Paper-red)
  - *Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, Ji-Rong Wen*
  - [Code üíª] [R1-Searcher](https://github.com/RUCAIBox/R1-Searcher) ![Star](https://img.shields.io/github/stars/RUCAIBox/R1-Searcher.svg?style=social&label=Star) 

### Agentic RAG
- [2502] **[Agentic Reasoning: Reasoning LLMs with Tools for the Deep Research](https://arxiv.org/pdf/2502.04644)** ![Arxiv](https://img.shields.io/badge/Arxiv-Paper-red)
  - *Junde Wu, Jiayuan Zhu, Yuyuan Liu*
  - [Code üíª] [Agentic-Reasoning](https://github.com/theworldofagents/Agentic-Reasoning) ![Star](https://img.shields.io/github/stars/theworldofagents/Agentic-Reasoning.svg?style=social&label=Star)

## Embedding Model
- [2502] **[O1 Embedder: Let Retrievers Think Before Action](https://arxiv.org/pdf/2502.07555)** ![Arxiv](https://img.shields.io/badge/Arxiv-Paper-red)
  - Ruiran Yan, Zheng Liu, Defu Lian